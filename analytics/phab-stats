#!/usr/bin/env python

# This script generates a set of productivity related metrics for a given
# Phabricator workboard and time range. The Phabricator workboard details
# must be specified via config file (see phab-stats-config.json). The time
# range is specified via start_date and end_date parameters.
#
# IMPORTANT: The script assumes that resolved tasks stay in the workboard
# 'forever'. This way, it can get the workboard via http, parse it and get
# all tasks that some day travelled through it.


import os
import re
import argparse
import json
import phabricator
import requests
from datetime import datetime, timedelta
import pylab as plt

DATE_FORMAT = '%Y-%m-%d'
IN_PROGRESS_COLUMN_VALUE = 2

def parse_arguments():
    parser = argparse.ArgumentParser(
        description='Generate metrics on Phabricator workboard ' +
                    'activity for a given date range.')
    def date_type(date_str):
        return datetime.strptime(date_str, DATE_FORMAT)
    parser.add_argument('start_date',
        help='Start of the interval for the generated summary (YYYY-MM-DD).',
        type=date_type)
    parser.add_argument('end_date',
        help='End of the interval for the generated summary (YYYY-MM-DD).',
        type=date_type)
    this_file_dir = os.path.dirname(os.path.realpath(__file__))
    parser.add_argument('--config',
        help='Location of the config file. Default: phab-stats-config.json.',
        default=os.path.join(this_file_dir, 'phab-stats-config.json'))
    return parser.parse_args()


def get_config(args):
    with open(args.config, 'r') as config_file:
        config = json.loads(config_file.read())
        # A map from column names to column values (scores).
        # Necessary to compute some metrics.
        config['column_values'] = {c['name']: c['value'] for c in config['columns']}
        # A map from column phab-ids to column names.
        # Necessary to understand Phabricator api response.
        config['column_names'] = {c['phid']: c['name'] for c in config['columns']}
    return config


class Task(object):
    """
    This class represents a Phabricator task.
    It also groups some convenient methods on task transactions.
    Phabricator stores any task update as a transaction. So, parsing
    a list of task transactions, lots of information can be extracted.
    """

    # The pattern that matches point definitions in task names.
    POINTS_REGEXP = re.compile(r'\[([0-9]+)\s*pts\]')

    def __init__(self, id, name):
        self.id = id
        self.name = name
        # The transaction list is asigned separately for performance reasons.
        self.transactions = None
        # The following fields depend on the transactions to be calculated.
        self.column = None
        self.points = None
        self.cached_start_date = None
        self.cached_resolved_date = None
        self.cached_created_date = None
        self.is_resolved = None

    def set_transactions(self, transactions):
        """
        Sets the task's transactions and triggers the
        calculation of the fields that depend on them.
        """
        self.transactions = transactions
        self.column = self.column_at('now')
        self.points = self.parse_points()

        # force is_resolved to get set
        self.resolved_date() # TODO find a cleaner way to do this

    def parse_points(self):
        """
        Sets the task's points by trying to parse them from its title,
        or otherwise looking at the transactions to calculate the point field.
        """
        match = Task.POINTS_REGEXP.search(self.name, re.I)
        if match:
            return int(match.group(1))
        for transaction in self.transactions:
            if transaction['transactionType'] == 'points':
                return transaction['newValue']
        return None

    def resolved_between(self, start_date, end_date):
        """
        Returns True if the task was resolved between the two dates.
        """
        resolved_date = self.resolved_date()
        if resolved_date:
            return (
                resolved_date >= start_date and
                resolved_date < end_date
            )
        else:
            return False

    def resolved_date(self):
        """
        Returns date when task was resolved.
        Returns None if not resolved.
        """

        if self.cached_resolved_date:
            return self.cached_resolved_date

        for transaction in self.transactions:
            if (transaction['transactionType'] == 'status' and
                    transaction['newValue'] == 'resolved'):
                timestamp = int(transaction['dateCreated'])
                self.cached_resolved_date = datetime.fromtimestamp(timestamp)
                self.is_resolved = True
                return self.cached_resolved_date
        self.is_resolved = False
        return None

    def start_date(self):
        """
        Returns date when task was put into In Progress column.
        If task was never moved to In Progress, returns None.
        """
        if self.cached_start_date:
            return self.cached_start_date

        for transaction in self.transactions:
            if transaction['transactionType'] == 'projectcolumn':
                columns = self.get_transaction_columns(transaction)
                if columns['newValue'] == IN_PROGRESS_COLUMN_VALUE:
                    timestamp = int(transaction['dateCreated'])
                    self.cached_start_date = datetime.fromtimestamp(timestamp)
                    return self.cached_start_date
        return None

    def cycle_time(self):
        return self.resolved_date() - self.start_date()

    def lead_time(self):
        return self.resolved_date() - self.created_date()

    def created_date(self):
        """
        Returns date when task was created.
        """
        if self.cached_created_date:
            return self.cached_created_date

        transaction = self.transactions[-1]
        timestamp = int(transaction['dateCreated'])
        self.cached_created_date = datetime.fromtimestamp(timestamp)
        return self.cached_created_date


    def column_at(self, date):
        """
        Returns the value of the column in which the task was at the given date.
        If the date is 'now', returns the column where the task is currently.
        """
        current_column = 0  # Board's initial column.
        # Transactions are sorted by timestamp, newest to oldest.
        # So they need to be reversed.
        for transaction in reversed(self.transactions):
            if transaction['transactionType'] == 'projectcolumn':
                timestamp = int(transaction['dateCreated'])
                transaction_date = datetime.fromtimestamp(timestamp)
                if date == 'now' or transaction_date <= date:
                    columns = self.get_transaction_columns(transaction)
                    current_column = columns['newValue']
                else:
                    # First column change after the given date.
                    # This means the current_column is the requested value.
                    break
        return current_column

    def steps_between(self, start_date, end_date):
        """
        Returns the number of column changes that a task has suffered,
        regardless if they are forwards or backwards. Only specified
        workboard columns do count. If a task jumps various steps in one
        transaction, they are counted as separate.
        """
        steps = 0
        for transaction in self.transactions:
            if transaction['transactionType'] == 'projectcolumn':
                columns = self.get_transaction_columns(transaction)
                steps += abs(columns['newValue'] - columns['oldValue'])
        return steps

    def get_transaction_columns(self, transaction):
        """
        Assumes the transaction is of type 'projectcolumn'.
        Returns the transaction's normalized old and new column values.
        """
        column_values = {}
        for kind in ['oldValue', 'newValue']:
            column_phids = transaction[kind]['columnPHIDs']
            # Phabricator api is crazy and returns different data types
            # for the same field. Sometimes list, sometimes dict.
            if type(column_phids) == list and len(column_phids) > 0:
                column_phid = column_phids[0]
            elif type(column_phids) == dict and len(column_phids) > 0:
                column_phid = column_phids.values()[0]
            else:
                column_values[kind] = 0 # Outside of workboard.
                continue
            # Translate Phabricator column phab-ids into column values.
            if column_phid in CONFIG['column_names']:
                column_name = CONFIG['column_names'][column_phid]
                column_values[kind] = CONFIG['column_values'][column_name]
            else:
                column_values[kind] = 0 # Outside of workboard.
        return column_values


def calculate_tasks_resolved(tasks, args):
    """
    Returns the number of tasks that have been
    marked as resolved between args.start_date and args.end_date.
    """
    tasks_resolved = 0
    for task in tasks:
        if task.resolved_between(args.start_date, args.end_date):
            tasks_resolved += 1
    return tasks_resolved

def calculate_timedeltas_average(timedeltas):
    seconds_list = [t.total_seconds() for t in timedeltas]
    average_seconds = sum(seconds_list) / float(len(seconds_list))
    return timedelta(seconds=average_seconds)

def calculate_points_moved_to_the_right(tasks, args):
    """
    For each task, gets the column it was placed at args.start_date and
    at args.end_date, and calculates how many columns the task has moved
    forward. For each step, adds 1/N of the task's points, where N is
    the total number of steps defined in the workboard. Returns the sum
    of it all.
    """
    total_columns = float(max([c['value'] for c in CONFIG['columns']]))
    points_moved_right = 0
    for task in tasks:
        initial_column = task.column_at(args.start_date)
        final_column = task.column_at(args.end_date)
        columns_moved = final_column - initial_column
        task_points = task.points or CONFIG['default_points']
        points_moved_right += columns_moved / total_columns * task_points
    return points_moved_right


def calculate_average_steps(tasks, args):
    """
    Returns the average number of column changes that the tasks marked
    as resolved between args.start_date and args.end_date have seen.
    Forward changes and backward changes all sum up.
    """
    steps_count = 0
    resolved_count = 0
    for task in tasks:
        if task.resolved_between(args.start_date, args.end_date):
            steps_count += task.steps_between(args.start_date, args.end_date)
            resolved_count += 1
    return steps_count / float(resolved_count) if resolved_count > 0 else 0

def print_cfd_table(tasks, args):
    """
    Prints CFD (Cumulative Flow Diagram) table
    """

    column_headers = '\t'.join(c['name'].ljust(15) for c in CONFIG['columns'][1:])
    print('Date\t\t' + column_headers)
    days_in_range = (args.end_date - args.start_date).days
    current_day = args.start_date
    for i in range(days_in_range):
        column_counts = joined_column_counts_for_day(current_day, tasks)
        print(current_day.strftime(DATE_FORMAT) + '\t' + column_counts)
        current_day += timedelta(days=1)

def joined_column_counts_for_day(day, tasks):
    return '\t'.join(str(c).ljust(15) for c in column_counts_for_day(day, tasks))

def show_cfd(tasks, args):
    days_in_range = (args.end_date - args.start_date).days
    dates = [(args.start_date + timedelta(days=d)) for d in range(days_in_range)]
    column_count = len(CONFIG['columns']) - 1
    Y = [[] for _ in range(column_count)]
    current_day = args.start_date
    for day_index in range(days_in_range):
        column_counts = column_counts_for_day(current_day, tasks)
        for column_index, column_count in enumerate(column_counts):
            Y[column_index].append(column_count)
        current_day += timedelta(days=1)

    plt.stackplot(dates, *Y, baseline="zero")
    plt.title("Cumulative Flow %s to %s" % (args.start_date.strftime(DATE_FORMAT), args.end_date.strftime(DATE_FORMAT)))
    plt.axis('tight')
    plt.show()

def column_counts_for_day(day, tasks):
    column_counts = [0] * (len(CONFIG['columns']) - 1)
    for task in tasks:
        if task.is_resolved:
            continue
        column_value = task.column_at(day)
        if column_value >= 1:
            column_counts[column_value - 1] += 1
    return column_counts


if __name__ == '__main__':
    args = parse_arguments()
    CONFIG = get_config(args)

    # Create the Phabricator API wrapper.
    phab = phabricator.Phabricator(timeout=500)

    # Get the tasks belonging to the given board.
    api_tasks = phab.maniphest.query(projectPHIDs=[CONFIG['workboard']])
    tasks = [
        Task(api_tasks[task_id]['id'], api_tasks[task_id]['title'])
        for task_id in api_tasks  # .values() not implemented
    ]

    # Decorate the tasks with their transactions.
    task_ids = [int(task.id) for task in tasks]
    transactions = phab.maniphest.gettasktransactions(ids=task_ids)
    for task in tasks:
        task.set_transactions(transactions[task.id])

    cycle_times = []
    lead_times = []

    # Calculate the metrics
    tasks_resolved = calculate_tasks_resolved(tasks, args)
    days_in_range = (args.end_date - args.start_date).days
    days_per_week = 7
    throughput = (tasks_resolved / float(days_in_range)) * days_per_week

    # Print task stats
    print('Date: %s - %s' % (args.start_date.strftime(DATE_FORMAT), args.end_date.strftime(DATE_FORMAT)))
    print('Completed Tasks (%d):' % tasks_resolved)
    print('ID\tCreated Date\t\tStart Date\t\tEnd Date\t\tCycle Time\t\tLead Time\t\tTask')
    for task in tasks:
        if task.resolved_between(args.start_date, args.end_date):
            created_date = str(task.created_date()).ljust(20)
            start_date = str(task.start_date()).ljust(20)
            if task.start_date():
                cycle_times.append(task.cycle_time())
                cycle_time = str(cycle_times[-1]).ljust(20)
            else:
                cycle_time = 'None'.ljust(20)
            lead_times.append(task.lead_time())
            lead_time = str(lead_times[-1]).ljust(20)
            print "%s\t%s\t%s\t%s\t%s\t%s\t%s" % (task.id, created_date, start_date, task.resolved_date(), cycle_time, lead_time, task.name)

    average_cycle_time = calculate_timedeltas_average(cycle_times)
    average_lead_time = calculate_timedeltas_average(lead_times)

    print('')
    print_cfd_table(tasks, args)

    # Print aggregate stats
    print('')
    print('Average cycle time: %s' % average_cycle_time)
    print('Average lead time: %s' % average_lead_time)
    print('Tasks completed per week (throughput): %.1f' % throughput)
    print('Note: average cycle time ignores tasks without a start date')

    show_cfd(tasks, args)
